{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4206a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc60fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Advertising.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8a72c",
   "metadata": {},
   "source": [
    "#### In this part we will import the different regression models to compare the performance for each of them (simple linear model, ridge regression and lasso regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27110c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6378c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = data.drop('sales', axis = 1)\n",
    "data_y = data['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c41ee",
   "metadata": {},
   "source": [
    "#### For the sake of this exercise, we will transform the dataset with polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b609676",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_transformer = PolynomialFeatures(degree = 3, include_bias = False)\n",
    "\n",
    "data_x = pd.DataFrame(poly_transformer.fit_transform(data_x),columns = poly_transformer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a6dc7ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b8fe24",
   "metadata": {},
   "source": [
    "#### In this part we will perform the feature scaling (standardization), to keep all the features with the same scale and prevent any of them to impact the predictions more than the other ones, and also, to be able to compare the features each other. Keep in mind that the scaler should be fitted in the X_train set ONLY, in order to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70b78d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "df_cols = data_x.columns\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns = df_cols)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns = df_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f4ac3",
   "metadata": {},
   "source": [
    "#### Let's define the following functions in order to modularize the modeling steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9966c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model,X_train,y_train):\n",
    "    model_object = model.fit(X_train,y_train)\n",
    "    return model_object\n",
    "def use_model(model, X_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    return predictions\n",
    "def evaluate_model(metric,predictions,y_test):\n",
    "    performance = metric(y_test, predictions)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7264a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define the linear models\n",
    "lin_regr = LinearRegression()\n",
    "ridge_regr = Ridge(alpha = 10)\n",
    "lasso_regr = Lasso(alpha = 10)\n",
    "models = [lin_regr, ridge_regr, lasso_regr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4acd1ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resulting_models = []\n",
    "for model in models:\n",
    "    resulting_models.append(fit_model(model, X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21115fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for model in resulting_models:\n",
    "    predictions.append(use_model(model, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3aae86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6607300",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for prediction in predictions:\n",
    "    mae = evaluate_model(mean_absolute_error, prediction, y_test)\n",
    "    rmse = np.sqrt(evaluate_model(mean_squared_error, prediction, y_test))\n",
    "    metrics.append((mae, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc1375ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.41275160852975196, 0.5803286825159616),\n",
       " (0.5774404204714175, 0.8946386461319681),\n",
       " (4.618428571428571, 5.399973733874139)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be4b72",
   "metadata": {},
   "source": [
    "#### So far we've tested linear regression vs ridge regression vs lasso regression, with a lambda value of 10 (this is given arbitrarily). The conclusion here is that the model performs better without regularization due to the lower RMSE corresponds to the linear regression model (in this case multiple linear regression).\n",
    "\n",
    "#### In theory this is not overfitting because the performance in both data sets is pretty well, and that's why the regularization methods lead to an increase in variance an then a lower performance.\n",
    "\n",
    "#### Now we will use cross validation to determine the best lambda for both regularization methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30653de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45f7ed",
   "metadata": {},
   "source": [
    "#### We will call the get_scorer_names module from sklearn.metrics in order to obtain the name of the scorers useful for the cross-validation process and then choose any of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffdc328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43f060f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'matthews_corrcoef',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_absolute_percentage_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'rand_score',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'top_k_accuracy',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "798b307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this case we've chosen negative mean squared error\n",
    "ridge_cv = RidgeCV(alphas = (0.1, 1, 10), scoring = 'neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa9d936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv.fit(X_train, y_train)\n",
    "predictions_ridge_cv = ridge_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5a3e28c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "321abf59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6180719926948917"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, predictions_ridge_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f938f7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42737748843414897"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, predictions_ridge_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518299a8",
   "metadata": {},
   "source": [
    "#### By modelling the Ridge regression algorithm with cross-validation, we obtained that the best lambda among 0.1, 1 and 10 is 0.1, because this resulted in a higher negative mean squared error and therefore, a better performance than before when we implemented the ridge regression with lambda = 10 (0.61 with lambda = 0.1 against 0.89 with lambda = 10).\n",
    "\n",
    "</br>\n",
    "\n",
    "#### In this last part we are going to do the same with the lasso regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4250b64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LassoCV(cv=5, eps=0.01, max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LassoCV</label><div class=\"sk-toggleable__content\"><pre>LassoCV(cv=5, eps=0.01, max_iter=10000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LassoCV(cv=5, eps=0.01, max_iter=10000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv = LassoCV(eps = 0.01, n_alphas = 100, max_iter = 10000, cv = 5)\n",
    "lasso_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "611e2c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04943070909225832"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e16825f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lasso_cv = lasso_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ec923ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7615440303260841"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, predictions_lasso_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d46ed02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5159788188265407"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, predictions_lasso_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661babd9",
   "metadata": {},
   "source": [
    "#### By looking at the metrics, we can see that the Lasso regression model performance improved a lot with respect to the previous model, which has a lambda  = 10 (RMSE = 5.39, lambda = 10 and RMSE = 0.76, lambda = 0.049).\n",
    "\n",
    "</br>\n",
    "\n",
    "#### Finally we will look at the coefficients. We can see that some of them are 0 and this is the difference with respect to ridge regression; in this case, even though the Lasso regression had a lower performance than ridge, it performed feature selection and left only the relevant variables which the huge difference between both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee903835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.35600233,  0.21183181,  0.        , -0.        ,  3.78675114,\n",
       "       -0.        ,  0.        ,  0.0624318 ,  0.        , -1.01152151,\n",
       "       -0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68d5c806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.40769392,  0.5885865 ,  0.40390395, -6.18263924,  4.59607939,\n",
       "       -1.18789654, -1.15200458,  0.57837796, -0.1261586 ,  2.5569777 ,\n",
       "       -1.38900471,  0.86059434,  0.72219553, -0.26129256,  0.17870787,\n",
       "        0.44353612, -0.21362436, -0.04622473, -0.06441449])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e754c1",
   "metadata": {},
   "source": [
    "### Now let's do the same for ElasticNet regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "65873b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "el_net_cv = ElasticNetCV(l1_ratio = np.arange(0.1,1.1,0.1), eps = 0.001, n_alphas = 100, max_iter = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "97d9c56a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ElasticNetCV(l1_ratio=array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       "             max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ElasticNetCV</label><div class=\"sk-toggleable__content\"><pre>ElasticNetCV(l1_ratio=array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       "             max_iter=10000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ElasticNetCV(l1_ratio=array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       "             max_iter=10000)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_net_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0478a90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004943070909225833"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_net_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "35dae5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_net_cv.l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d6e7998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_predictions = el_net_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0495917c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6063140748984046"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, elastic_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dce813af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4335034618590077"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, elastic_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db62af60",
   "metadata": {},
   "source": [
    "#### We can see now that the ElasticNet regression improved the model performance, even more than Lasso regression did. by combining every parameter; in l1_ratio, it tested values between 0 and 1, with a path length of 0.1, and a lambda value from 0.001 until 100 with a path length of 0.001. \n",
    "</br>\n",
    "\n",
    "#### Given that the nature of ElasticNet regression is integrating both regularization, we could think of this function as an 'automatic' way of finding the best regularization method and its parameters. In our case it selected an l1_ratio = 1, which means that the best model is actually l1. Otherwise, it would have adjusted a model with the better alpha (lambda) for both regularization terms in the equation an their respective weights (l1_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
